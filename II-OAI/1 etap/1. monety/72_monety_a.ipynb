{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChtPHCSkTM5e"
   },
   "source": [
    "# Maszynka do Liczenia Monet\n",
    "\n",
    "![coin_counter_image.png](https://live.staticflickr.com/65535/54326911769_4e446cdd97_k.jpg)\n",
    "*Obraz wygenerowany przy użyciu modelu Flux-dev.*\n",
    "\n",
    "## Wstęp\n",
    "Wyobraź sobie, że na strychu odkrywasz duży kufer pełen monet i postanawiasz policzyć ich łączną wartość. Ręczne przeliczanie każdej z monet byłoby czasochłonne i nudne, dlatego decydujesz się na zautomatyzowanie rozwiązania. Na podstawie zdjęć rozrzuconych monet, twoim celem jest określenie ich wspólnej wartości.\n",
    "Rozbijając to zadanie na czynniki pierwsze, Twoim zadaniem będzie:\n",
    "* zlokalizowanie każdej monety na zdjęciu,\n",
    "* przypisanie ich do odpowiednich kategorii według nominałów (np. 1 grosz czy 2 złote).\n",
    "\n",
    "Zadanie to w dziedzinie wizji komputerowej nazywamy detekcją obiektów. Zwróć uwagę, że zadanie detekcji oznacza jednoczesne zadanie klasyfikacji, jak i regresji - musisz bowiem określić zarówno klasę obiektu, jak i jego położenie na zdjęciu. Dodatkowym wyzwaniem, który odróżnia detekcję od zwykłego zadania klasyfikacji jest fakt, że zamiast jednego obiektu reprezentującego jedną z klas, na zdjęciu może znajdować się wiele obiektów różnych klas.\n",
    "\n",
    "Formalnie możemy powiedzieć, że rozwiązaniem $\\hat{\\mathcal{B}}$ powinien być zbiór krotek, zawierających, kolejno, współrzędne prostokąta, w którym znajduje się moneta, etykietę, która określa nominał monety, a także pewność, z jaką model jest przekonany, że znalazł monetę.\n",
    "$$\n",
    "\\hat{\\mathcal{B}} = \\{(x_{min}, y_{min}, x_{max}, y_{max}, c, \\phi), \\dots\\}\n",
    "$$\n",
    "gdzie $(x_{min}, y_{min})$ to lewy górny róg prostokąta, $(x_{max}, y_{max})$ to prawy dolny róg prostokąta, $c$ to etykieta monety, a $\\phi$ to pewność modelu co do predykcji. W zależności od zdjęcia, zbiór $\\hat{\\mathcal{B}}$ może posiadać różną ilość elementów, odpowiadającą ilości znalezionych monet. Kolejność monet w zbiorze nie ma znaczenia.\n",
    "\n",
    "*Uwaga*: Przy pozycjach prostokątów na zdjęciu, warto zwrócić uwagę, że punkt (0, 0) znajduje się w lewym górnym rogu obrazu, a oś Y rośnie w dół. Jest to standardowy układ współrzędnych stosowany w grafice komputerowej, który różni się od układu współrzędnych stosowanego w matematyce.\n",
    "\n",
    "Podczas wykonywania detekcji możemy popełnić pięć rodzajów błędów, które zostały przedstawione na poniższych obrazkach:\n",
    "- Model wykrył obiekt, w miejscu gdzie go nie ma (taką sytuację nazywamy False Positive)\n",
    "- Model nie wykrył obiektu, który jest na obrazie (taką sytuację nazywamy False Negative)\n",
    "- Model wykrył obiekt, ale niepoprawnie go zaklasyfikował\n",
    "- Model poprawnie wykrył i zaklasyfikował obiekt, ale zwrócił nieprecyzyjne współrzędne prostokąta\n",
    "- Model wykrył ten sam obiekt kilka razy\n",
    "\n",
    "![coin_counter_prediction.png](https://live.staticflickr.com/65535/54327101055_460ce3640e_k.jpg)\n",
    "\n",
    "W tym przypadku bierzemy pod uwagę tylko polskie monety i ograniczamy się do stron nominalnych. Zarówno banknoty, jak i strona z wizerunkiem orła nie są zawarte w zbiorze danych i nie są brane pod uwagę.\n",
    "\n",
    "[Link](https://github.com/OlimpiadaAI/szkolenia/blob/main/12_Zadania_detekcji_i_segmentacji.pdf) do slajdów z wykładu na temat detekcji i segmentacji obiektów.\n",
    "\n",
    "## Zadanie\n",
    "Twoim zadaniem jest zaimplementowanie klasy ```YourDetector```, w której metoda ```forward``` przyjmuje obraz i zwraca zbiór zawierający predykcje monet w formacie opisanym przy definicji zbioru $\\hat{\\mathcal{B}}$.\n",
    "\n",
    "### Kryterium Oceny\n",
    "*Uwaga*: Zarówno metody obliczające metryki, jak i ładowanie danych zostały zaimplementowane w zadaniu. Twoim jedynym celem jest zaimplementowanie modelu detekcji, jednak zachęcamy do zapoznania się z opisem metryk, aby lepiej zrozumieć problem.\n",
    "\n",
    "Zadanie zostanie ocenione na podstawie metryki [mAP](https://kili-technology.com/data-labeling/machine-learning/mean-average-precision-map-a-complete-guide) (ang. *mean Average Precision*), która jest standardową metryką w dziedzinie detekcji obiektów.  \n",
    "Wyznaczenie tej metryki zaczyna się od sparowania predykcji z prawdziwymi obiektami. W tym celu wykorzystuje się metrykę IoU (ang. *Intersection over Union*), która określa stopień pokrycia dwóch prostokątów. Wartość IoU jest zdefiniowana jako stosunek pola wspólnego dla obu prostokątów $A_{\\text{inter}}$ do pola ich sumy $A_{\\text{union}}$.\n",
    "$$ IoU = \\frac{A_{\\text{inter}}}{A_{\\text{union}}} $$\n",
    "Patrząc na poniższy rysunek, możemy powiedzieć, że IoU to stosunek pola żółtego do pola niebieskiego. Warto zauważyć, że jeśli prostokąty nie miałyby części wspólnej, to wartość IoU wynosiłaby $0$, a jeśli oba prostokąty byłyby identyczne, to wartość IoU wynosiłaby $1$.\n",
    "Wartość IoU musi przekroczyć ustalony próg, abyśmy mogli uznać, że dwa prostokąty się pokrywają. Im wyższy próg, tym bardziej dokładne muszą być wskazane lokalizacje obiektów przez model, żeby zostały one sparowane z prostokątami utworzonymi na podstawie rzeczywistych współrzędnych.\n",
    "\n",
    "![coin_counter_explanation_1.png](https://live.staticflickr.com/65535/54326929064_3927b29b3b_k.jpg)\n",
    "\n",
    "Po dopasowaniu predykcji modelu do rzeczywistych obiektów możemy obliczyć dwie kluczowe miary: precyzję (ang. *precision*) oraz czułość (ang. *recall*). Obie miary liczymy dla każdej klasy z osobna.\n",
    "- Precyzja wyraża odsetek obiektów poprawnie sklasyfikowanych do danej klasy wśród wszystkich obiektów, które zostały do niej przypisane.\n",
    "- Czułość opisuje odsetek prawidłowo sklasyfikowanych obiektów danej klasy w stosunku do wszystkich obiektów tej konkretnej klasy.\n",
    "\n",
    "Te dwie miary pomagają ocenić skuteczność modelu. Jednak ponieważ modele są niedoskonałe i popełniają różnego rodzaju błędy, próba poprawy jednej z metryk wiąże się z reguły z pogorszeniem drugiej. Gdy dla przykładu zmniejszymy próg pewności modelu, będziemy brać pod uwagę większą liczbę obiektów wskazanych przez model, ale jednocześnie może to zwiększyć ilość obiektów fałszywie wskazanych przez model za istotne.\n",
    "\n",
    "Aby lepiej zrozumieć ten kompromis, korzystamy z wartości pewności modelu (oznaczanej jako $\\phi$) i wyznaczamy na ich podstawie krzywą precision-recall. Ta krzywa pokazuje, jak zmieniają się precyzja i czułość w zależności od ustawionego progu pewności.\n",
    "\n",
    "Pole pod poniżej narysowaną krzywą to średnia precyzja (AP) dla danej klasy.\n",
    "\n",
    "![coin_counter_explanation_2.png](https://live.staticflickr.com/65535/54326928103_12e06df704_z.jpg)\n",
    "\n",
    "Metryka mAP, to średnia precyzja dla wszystkich klas, czyli średnie pole pod krzywą precision-recall dla wszystkich klas, opisane wzorem:\n",
    "$$ mAP = \\frac{1}{K} \\sum_{k=1}^K {AP}_{k}, $$\n",
    "gdzie $K$ to liczba klas, a ${AP}_{k}$ to średnia precyzja dla klasy $k$.\n",
    "\n",
    "Ostatnim krokiem jest wybór wartości IoU, dla której chcemy wyznaczyć mAP. Standardową praktyką, którą wykorzystamy do ocenienia twojego rozwiązania, jest wyznaczanie mAP dla różnych wartości IoU (zaczynając od 0.5 i zwiększając wartość o 0.05 aż do 0.95) i uśrednianie wyników. Dzięki temu metryka lepiej odzwierciedla nie tylko jakość klasyfikacji ale także jakość lokalizacji obiektów (dla progu IoU=0.95 model musi zwracać predykcje niemal idealnie pokrywające się z docelowymi prostokątami, a w przypadku IoU=0.5 metryka \"wybacza\" znacznie większe różnice w położeniu).\n",
    "\n",
    "**Ostatecznie Twoje rozwiązanie oceniane będzie na tajnym zbiorze testowym na podstawie metryki mAP.** Zbiór testowy nie różni się znacząco od zbioru walidacyjnego.\n",
    "\n",
    "- Gdy wartość mAP dla Twojego modelu będzie wynosiła 0.2 (lub poniżej), otrzymasz 0 punktów za zadanie.\n",
    "- Gdy wartość mAP dla Twojego modelu będzie wynosiła 0.85 (lub powyżej), otrzymasz 100 punktów za zadanie.\n",
    "- W przeciwnym wypadku, liczba punktów zostanie wyznaczona proporcjonalnie do wartości mAP:\n",
    "$$\n",
    "\\text{score} = \\frac{mAP - 0.2}{0.85 - 0.2} \\times 100\n",
    "$$\n",
    "\n",
    "\n",
    "## Ograniczenia\n",
    "- Twoje rozwiazanie będzie testowane na Platformie Konkursowej bez dostępu do internetu oraz w środowisku z GPU.\n",
    "- Ewaluacja Twojego finalnego rozwiązania na Platformie Konkursowej nie może trwać dłużej niż 10 minut z GPU, natomiast ewaluacja dla pojedyńczego zdjęcia nie może trwać dłużej niż 5 sekund.\n",
    "- Model nie może korzystać z innych zbiorów danych oraz z pre-trenowanych wag na innych zbiorach danych.\n",
    "- Model musi zwracać wyniki w formacie, który jest kompatybilny z funkcją ```predict_all_bounding_boxes``` (podobnie jak przykładowe rozwiązanie).\n",
    "- Model musi dziedziczyć po klasie ```nn.Module```.\n",
    "\n",
    "## Pliki Zgłoszeniowe\n",
    "Ten notebook uzupełniony o Twoje rozwiązanie (patrz klasa `YourDetector`).\n",
    "\n",
    "## Ewaluacja\n",
    "Pamiętaj, że podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`.\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy 0 a 100 punktów. Liczba punktów, którą zdobędziesz, będzie wyliczona na (tajnym) zbiorze testowym na Platformie Konkursowej na podstawie wyżej wspomnianego wzoru, zaokrąglona do liczby całkowitej. Jeśli Twoje rozwiązanie nie będzie spełniało powyższych kryteriów lub nie będzie wykonywać się prawidłowo, otrzymasz za zadanie 0 punktów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFfTZvjkuScl"
   },
   "source": [
    "# Kod Startowy\n",
    "W tej sekcji inicjalizujemy środowisko poprzez zaimportowanie potrzebnych bibliotek i funkcji. Przygotowany kod ułatwi Tobie efektywne operowanie na danych i budowanie właściwego rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kbilZQOVTM5i"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "FINAL_EVALUATION_MODE = False  # Podczas sprawdzania ustawimy tą flagę na True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xkmsLt9NuScl"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import gdown\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torchvision.transforms.v2 as T\n",
    "from collections.abc import Callable\n",
    "from matplotlib import patches\n",
    "from matplotlib.collections import PatchCollection\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.ops import box_iou\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA niedostępna!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hq9ZC3gDuScm"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "seed = 12345\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZtACv_yjuScm"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "# Komórka zawierająca funkcje pomocnicze do przygotowania danych.\n",
    "\n",
    "class CoinsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Zbiór danych monet wczytywany z pliku pickle.\n",
    "\n",
    "    Przyjmuje:\n",
    "        pickle_file (str): Ścieżka do pliku pickle zawierającego dane.\n",
    "        transform (callable, opcjonalnie): Transformacje stosowane do obrazów i etykiet.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pickle_file: str,\n",
    "            transform: Callable | None = None\n",
    "        ):\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Zwraca liczbę próbek w zbiorze danych.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(\n",
    "            self,\n",
    "            idx: int\n",
    "        ) -> dict:\n",
    "        \"\"\"\n",
    "        Pobiera próbkę danych na podstawie indeksu.\n",
    "\n",
    "        Przyjmuje:\n",
    "            idx (int): Indeks próbki.\n",
    "\n",
    "        Zwraca:\n",
    "            dict: Słownik zawierający obraz oraz odpowiadające mu obiekty docelowe (boxes, labels).\n",
    "        \"\"\"\n",
    "        sample = self.data[idx]\n",
    "        image = sample['image']\n",
    "        target = {\n",
    "            'boxes': sample['boxes'],\n",
    "            'labels': sample['labels']\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            **target\n",
    "        }\n",
    "\n",
    "\n",
    "def setup_data(\n",
    "        train_transform: Callable | None = None,\n",
    "        val_transform: Callable | None = None,\n",
    "        root: str = 'data/'\n",
    "    ) -> tuple:\n",
    "    \"\"\"\n",
    "    Przygotowuje zbiory danych do trenowania i walidacji, pobierając je jeśli to konieczne.\n",
    "\n",
    "    Przyjmuje:\n",
    "        train_transform (callable, opcjonalnie): Augmentacje dla zbioru treningowego.\n",
    "        val_transform (callable, opcjonalnie): Augmentacje dla zbioru walidacyjnego.\n",
    "        root (str, opcjonalnie): Katalog bazowy dla plików z danymi.\n",
    "\n",
    "    Zwraca:\n",
    "        tuple: Zbiory danych (train_ds, val_ds).\n",
    "    \"\"\"\n",
    "    if train_transform is None:\n",
    "        train_transform = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale=True)])\n",
    "    if val_transform is None:\n",
    "        val_transform = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale=True)])\n",
    "\n",
    "    train_file = root+'train.pkl'\n",
    "    val_file = root+'val.pkl'\n",
    "\n",
    "    if not os.path.exists(root):\n",
    "        os.makedirs(root)\n",
    "\n",
    "    if not os.path.exists(train_file):\n",
    "        url = \"https://drive.google.com/uc?id=1KC8FBlCuwh9ITUt0CcPeRJCpqy5j4WBp\"\n",
    "        gdown.download(url, train_file, quiet=True)\n",
    "\n",
    "    if not os.path.exists(val_file):\n",
    "        url = \"https://drive.google.com/uc?id=1Oza4UjnmAUeae2cA8YDwMWxVHOb7SKdP\"\n",
    "        gdown.download(url, val_file, quiet=True)\n",
    "\n",
    "    train_ds = CoinsDataset(root+'train.pkl', transform=train_transform)\n",
    "    val_ds = CoinsDataset(root+'val.pkl', transform=val_transform)\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B5tcTzZ8uScm"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "# Komórka zawierająca funkcje pomocnicze do wyznaczenia metryk oceniających jakość modelu.\n",
    "\n",
    "def predict_all_bounding_boxes(\n",
    "        model: nn.Module,\n",
    "        ds: Dataset\n",
    "    ) -> list:\n",
    "    \"\"\"\n",
    "    Funkcja przewidująca wszystkie bounding boxy dla zbioru danych z wykorzystaniem modelu.\n",
    "\n",
    "    Przyjmuje:\n",
    "        model: Model detekcji obiektów.\n",
    "        ds: Zbiór danych.\n",
    "\n",
    "    Zwraca:\n",
    "        Lista zawierająca wszystkie przewidziane bounding boxy dla każdej próbki w zbiorze danych.\n",
    "    \"\"\"\n",
    "    all_pred_bboxes = []\n",
    "\n",
    "    for sample in ds:\n",
    "        img = sample[\"image\"].to(DEVICE)\n",
    "        pred_bboxes = model(img)\n",
    "        all_pred_bboxes.append(pred_bboxes)\n",
    "\n",
    "    return all_pred_bboxes\n",
    "\n",
    "\n",
    "def calculate_map(\n",
    "        predictions: list,\n",
    "        ds: Dataset,\n",
    "        return_all: bool = False\n",
    "    ) -> dict | float:\n",
    "    \"\"\"\n",
    "    Funkcja obliczająca średnią precyzję średnią (mAP) dla przewidzianych bounding boxów dla całego zbioru danych.\n",
    "\n",
    "    Przyjmuje:\n",
    "        predictions: Lista zawierająca przewidziane bounding boxy dla każdej próbki w zbiorze danych.\n",
    "        ds: Zbiór danych, zawierający prawdziwe bounding boxy.\n",
    "        return_all (bool, opcjonalnie): Czy zwrócić wszystkie metryki, czy tylko mAP:0.5:0.95:0.05.\n",
    "\n",
    "    Zwraca:\n",
    "        Wartość mAP lub wszystkie metryki w formie słownika.\n",
    "    \"\"\"\n",
    "    meta = []\n",
    "\n",
    "    for img_meta in predictions:\n",
    "        entry = {\n",
    "            \"boxes\": [],\n",
    "            \"labels\": [],\n",
    "            \"scores\": []\n",
    "        }\n",
    "\n",
    "        for box in img_meta:\n",
    "            entry[\"boxes\"].append(box[:4])\n",
    "            entry[\"labels\"].append(box[4])\n",
    "            entry[\"scores\"].append(box[5])\n",
    "\n",
    "        meta.append(entry)\n",
    "\n",
    "    for i in range(len(meta)):\n",
    "        meta[i]['boxes'] = torch.tensor(meta[i]['boxes'])\n",
    "        meta[i]['labels'] = torch.tensor(meta[i]['labels']).view(-1)\n",
    "        meta[i]['scores'] = torch.tensor(meta[i]['scores'])\n",
    "\n",
    "    mAP = MeanAveragePrecision()\n",
    "\n",
    "    GT = [{\n",
    "        \"boxes\": sample[\"boxes\"],\n",
    "        \"labels\": sample[\"labels\"]\n",
    "    } for sample in ds]\n",
    "\n",
    "    output = mAP(meta, GT)\n",
    "\n",
    "    if return_all:\n",
    "        return output\n",
    "\n",
    "    return mAP(meta, GT)['map'].item()\n",
    "\n",
    "def compute_confusion_matrix(\n",
    "        predictions: list,\n",
    "        ds: Dataset,\n",
    "        iou_threshold: float = 0.5\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Funkcja obliczająca macierz pomyłek dla przewidzianych bounding boxów dla całego zbioru danych.\n",
    "\n",
    "    Przyjmuje:\n",
    "        predictions: Lista zawierająca przewidziane bounding boxy dla każdej próbki w zbiorze danych.\n",
    "        ds: Zbiór danych, zawierający prawdziwe bounding boxy.\n",
    "        iou_threshold (float, opcjonalnie): Próg IoU dla przypisania predykcji do obiektu.\n",
    "\n",
    "    Zwraca:\n",
    "        Macierz pomyłek.\n",
    "    \"\"\"\n",
    "    num_classes = 10  # 9 klas monet + 1 tło\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    for pred_boxes, item in zip(predictions, ds):\n",
    "        # Ground truth\n",
    "        gt_boxes = item['boxes']\n",
    "        gt_labels = item['labels']\n",
    "\n",
    "        # predykcje\n",
    "        pred_boxes_tensor = torch.tensor([p[:4] for p in pred_boxes]) if pred_boxes else torch.empty((0, 4))\n",
    "        pred_labels = torch.tensor([p[4] for p in pred_boxes]) if pred_boxes else torch.empty((0,), dtype=torch.long)\n",
    "\n",
    "        # wartości IoU dla wszystkich par predykcji i ground truth\n",
    "        iou_matrix = box_iou(pred_boxes_tensor, gt_boxes) if pred_boxes else torch.empty((0, gt_boxes.shape[0]))\n",
    "\n",
    "        # na podstawie IoU przypisz predykcje do ground truth\n",
    "        matched_gt = set()\n",
    "        for pred_idx, ious in enumerate(iou_matrix):\n",
    "            max_iou, gt_idx = torch.max(ious, dim=0)\n",
    "            if max_iou >= iou_threshold and gt_idx.item() not in matched_gt:\n",
    "                conf_matrix[gt_labels[gt_idx].item(), pred_labels[pred_idx].item()] += 1\n",
    "                matched_gt.add(gt_idx.item())\n",
    "            else:\n",
    "                conf_matrix[-1, pred_labels[pred_idx].item()] += 1  # False positive\n",
    "\n",
    "        # Dla wszystkich nieprzypisanych obiektów ground truth dodaj jako False negative\n",
    "        for gt_idx in range(len(gt_boxes)):\n",
    "            if gt_idx not in matched_gt:\n",
    "                conf_matrix[gt_labels[gt_idx].item(), -1] += 1\n",
    "\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nTxTIcfJuScm"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "# Komórka zawierająca funkcje pomocnicze do wizualiacji wyników\n",
    "\n",
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"purple\", \"orange\", \"brown\", \"pink\"]\n",
    "label_names = ['1 grosz', '2 grosze', '5 groszy', '10 groszy', '20 groszy', '50 groszy', '1 złotych', '2 złote', '5 złotych']\n",
    "\n",
    "def show_sample(\n",
    "        sample: dict\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Funkcja wyświetlająca obraz z bounding boxami obiektów.\n",
    "\n",
    "    Przyjmuje:\n",
    "        sample: Słownik zawierający obraz oraz etykiety.\n",
    "    \"\"\"\n",
    "    image = sample['image']\n",
    "    meta = sample\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.imshow(image.permute((1, 2, 0)))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if meta is not None:\n",
    "        patches_list = []\n",
    "        legend_labels = []\n",
    "\n",
    "        for bbox, label in zip(meta['boxes'], meta['labels']):\n",
    "            points = np.array(bbox)\n",
    "            points = points.astype(int)\n",
    "\n",
    "            # Rysowanie prostokąta, który otacza obiekt\n",
    "            rect = patches.Rectangle(\n",
    "                (points[0], points[1]),\n",
    "                points[2] - points[0],\n",
    "                points[3] - points[1],\n",
    "                linewidth=2,\n",
    "                edgecolor=colors[label.item()],\n",
    "                facecolor='none'\n",
    "            )\n",
    "            patches_list.append(rect)\n",
    "\n",
    "            # legenda z unikalnymi etykietami\n",
    "            if label_names[label.item()] not in legend_labels:\n",
    "                legend_labels.append(label_names[label.item()])\n",
    "\n",
    "        patch_collection = PatchCollection(patches_list, match_original=True)\n",
    "        plt.gca().add_collection(patch_collection)\n",
    "\n",
    "        # Dodanie legendy z unikalnymi etykietami\n",
    "        handles = [patches.Patch(color=colors[i], label=label) for i, label in enumerate(label_names) if label in legend_labels]\n",
    "        plt.legend(handles=handles, loc=\"upper right\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_detection_results_grid(\n",
    "        predictions: list,\n",
    "        ds: Dataset,\n",
    "        size: tuple = (2, 3)\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Funkcja wyświetlająca wyniki detekcji na wybranych zdjęciach walidacyjnych.\n",
    "\n",
    "    Przyjmuje:\n",
    "        predictions: Lista zawierająca przewidziane bounding boxy dla każdej próbki w zbiorze danych.\n",
    "        ds: Zbiór danych.\n",
    "        size (tuple, opcjonalnie): Rozmiar siatki obrazów.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(size[0], size[1], figsize=(size[1]*6, size[0]*5))\n",
    "\n",
    "    for i, (sample, pred_meta) in enumerate(zip(ds, predictions)):\n",
    "        if i >= size[0] * size[1]:\n",
    "            break\n",
    "\n",
    "        img = sample[\"image\"]\n",
    "\n",
    "        ax = axes[i // size[1], i % size[1]]\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "\n",
    "        for x1, y1, x2, y2, label, _ in pred_meta:\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1),\n",
    "                x2 - x1,\n",
    "                y2 - y1,\n",
    "                linewidth=2,\n",
    "                edgecolor=colors[label],\n",
    "                facecolor='none',\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    handles = [patches.Patch(color=colors[i], label=label) for i, label in enumerate(label_names)]\n",
    "    plt.legend(handles=handles, loc='upper right')\n",
    "    plt.suptitle(\"Detekcja monet na wybranych sześciu zdjęciach walidacyjych\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "        predictions: list,\n",
    "        ds: Dataset,\n",
    "        iou_threshold: float = 0.5\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Funkcja wyświetlająca macierz pomyłek dla detekcji monet.\n",
    "\n",
    "    Przyjmuje:\n",
    "        predictions: Wynik detekcji monet.\n",
    "        ds: Zbiór danych.\n",
    "        iou_threshold float: Próg IoU dla przypisania predykcji do obiektu\n",
    "    \"\"\"\n",
    "    conf_matrix = compute_confusion_matrix(predictions, ds, iou_threshold=iou_threshold)\n",
    "\n",
    "    labels = [\"1 grosz\", \"2 grosze\", \"5 groszy\", \"10 groszy\", \"20 groszy\", \"50 groszy\", \"1 złotych\", \"2 złote\", \"5 złotych\", \"brak (tło)\"]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels, cbar=False)\n",
    "\n",
    "    plt.xlabel(\"Model predykował\", labelpad=15)\n",
    "    plt.ylabel(\"podczas, gdy powinien był predykować\", labelpad=15)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.title(\"Macierz pomyłek dla detekcji monet z użyciem IOU={}\".format(iou_threshold))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCM4sFoFuScm"
   },
   "source": [
    "## Ładowanie Danych\n",
    "Za pomocą poniższego kodu dane zostaną wczytane i odpowiednio przygotowane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f6Rywvl-TM5m"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "train_ds, val_ds = setup_data(root='./data/')\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    print(\"Ilość zdjęć w zbiorze treningowym:\", len(train_ds), \", ilość zdjęć w zbiorze walidacyjnym:\", len(val_ds))\n",
    "\n",
    "    sample = train_ds[0]\n",
    "\n",
    "    print(\"Każda próbka zawiera:\", list(sample.keys()))\n",
    "    print(\"Każde zdjęcie ma wymiary:\", list(sample[\"image\"].shape))\n",
    "    print(\"Ta próbka posiada\", sample['labels'].shape[0], \"obiektów\")\n",
    "    print(\"Każdy prostokąt jest opisany jako\", sample['boxes'].shape[1], \"wartości (x1, y1, x2, y2)\")\n",
    "\n",
    "    show_sample(sample) # wyświetlenie przykładowego zdjęcia z zaznaczonymi obiektami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufZEuXrsTM5n"
   },
   "source": [
    "### Klasy Obiektów i ich Etykiety\n",
    "Poniżej znajduje się tabelka z etykietami klas znajduących się w zbiorze danych wraz z ich krótkim opisem.\n",
    "\n",
    "| Etykieta | Opis |\n",
    "| --- | --- |\n",
    "| 0 | Moneta o nominale 1 grosza |\n",
    "| 1 | Moneta o nominale 2 grosze |\n",
    "| 2 | Moneta o nominale 5 groszy |\n",
    "| 3 | Moneta o nominale 10 groszy |\n",
    "| 4 | Moneta o nominale 20 groszy |\n",
    "| 5 | Moneta o nominale 50 groszy |\n",
    "| 6 | Moneta o nominale 1 złoty |\n",
    "| 7 | Moneta o nominale 2 złote |\n",
    "| 8 | Moneta o nominale 5 złotych |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dliLdfUHTM5o"
   },
   "source": [
    "## Przykładowe Rozwiązanie\n",
    "Poniżej przedstawiamy uproszczone rozwiązanie, które służy jako przykład demonstrujący podstawową funkcjonalność notatnika. Może ono posłużyć jako punkt wyjścia do opracowania Twojego rozwiązania.\n",
    "\n",
    "Jako prosty przykład może posłuży sieć konwolucyjna aplikowana na okno przesuwne. Metoda ta polega na wyciąganiu fragmentów zdjęcia (w naszym przypadku o rozmiarach typowych dla monet w zbiorze danych) i klasyfikacji każdego z nich przez sieć konwolucyjną. Sieć ta na wyjściu zwraca jedną z 10 klas: dziewięć z nich należy do różnych nominałów, natomiast ostatnia klasa jest zarezerwowania dla tła, czyli miejsca gdzie nie ma żadnej monety. Operację klasyfikacji przeprowadzamy dla każdego fragmentu zdjęcia, przesuwając się o określoną wartość. W przypadku, gdy sieć zwraca klasę tła, pomijamy ten fragment, w przeciwnym przypadku, do analizowanego fragmentu przypisujemy etykietę monety. W ten sposób otrzymujemy zbiór prostokątów, które zawierają monety.\n",
    "\n",
    "Przy implementacji należy wziąć pod uwagę dodatkową klasę, która będzie reprezentować tło (prostokąt, w którym nie ma żadnej monety). W procesie treningu możemy robić losowe wycinki zdjęcia. Jeśli wycinek pokrywa się z monetą z wartością IoU wyższą niż $0.5$, to wybieramy jej nominał jako etykietę. W przypadku gdy wycinek nie pokrywa się z żadną monetą - wybieramy tło jako etykietę.\n",
    "\n",
    "Zaczniemy od zdefiniowania prostego modelu, a w następnych komórkach zajmiemy się implementacją funkcji trenującej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vlafKX9uTM5p"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "class BasicCNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNNClassifier, self).__init__()\n",
    "        self.net = resnet18(weights=None, num_classes=10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Funkcja klasyfikująca obraz x do jednej z 10 klas: 9 monet + tło.\n",
    "\n",
    "        Przyjmuje:\n",
    "            x (torch.Tensor): Obraz do klasyfikacji.\n",
    "\n",
    "        Zwraca:\n",
    "            torch.Tensor: Predykcje modelu.\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9LKDky1STM5q"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "# kod do przerabiania zdjęcia z wieloma monetami do danych uczących dla klasyfikatora\n",
    "# w 90% przypadków losujemy obiekt z obrazka i wycinamy jego otoczenie\n",
    "# w 10% przypadków wybieramy zupełnie losowy fragment obrazka\n",
    "\n",
    "class ClassificationDataPreprocessor:\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_of_crops_per_image: int = 4,\n",
    "            box_size: int = 64\n",
    "        ):\n",
    "        self.num_of_crops_per_image = num_of_crops_per_image\n",
    "        self.box_size = box_size\n",
    "\n",
    "    def get_label_of_crop(\n",
    "            self,\n",
    "            crop_box: tuple,\n",
    "            boxes: torch.Tensor,\n",
    "            labels: torch.Tensor,\n",
    "            iou_threshold: float = 0.5\n",
    "        ) -> int:\n",
    "        \"\"\"\n",
    "        Funkcja zwracająca etykietę dla wycinka obrazu w zależności od tego, czy pokrywa się z monetą z wartością IoU > 0.5.\n",
    "\n",
    "        Przyjmuje:\n",
    "            crop_box (tuple): Coordynaty lewego-górnego oraz prawego-dolnego rogu wycinka obrazu (x1, y1, x2, y2).\n",
    "            boxes (torch.Tensor): Tensor zawierający bounding boxy monet.\n",
    "            labels (torch.Tensor): Tensor zawierający etykiety monet.\n",
    "            iou_threshold (float): Próg IoU dla przypisania predykcji do obiektu.\n",
    "        \"\"\"\n",
    "        for box, label in zip(boxes, labels):\n",
    "            if box_iou(torch.tensor(crop_box).view(1, 4), box.view(1, 4)) > iou_threshold:\n",
    "                return label\n",
    "        return 9 # wartości 0-8 to etykiety monet, dlatego 9 będzie tłem\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            batch: list\n",
    "        ) -> dict:\n",
    "        \"\"\"\n",
    "        Funkcja przetwarzająca obrazy z wieloma monetami na dane uczące dla klasyfikatora, zawierające wycinki obrazów z ich etykietami.\n",
    "\n",
    "        Przyjmuje:\n",
    "            batch (list): Lista słowników zawierających obrazy, bounding boxy monet oraz ich etykiety.\n",
    "\n",
    "        Zwraca:\n",
    "            dict: Słownik zawierający wycinki obrazów oraz ich etykiety.\n",
    "        \"\"\"\n",
    "        crops, labels = [], []\n",
    "\n",
    "        for sample in batch:\n",
    "            img = sample['image']\n",
    "\n",
    "            for _ in range(self.num_of_crops_per_image):\n",
    "                if torch.rand(1) < 0.1: # raz na 10 prób losujemy tło (staramy się w ten sposób zbalansować zbiór)\n",
    "                    central_x = torch.randint(0, img.shape[2], (1,)).item()\n",
    "                    central_y = torch.randint(0, img.shape[1], (1,)).item()\n",
    "                else: # w pozostałych 90% losujemy obiekt z obrazka i wycinamy jego otoczenie\n",
    "                    idx_gt = torch.randint(0, sample['labels'].shape[0], (1,)).item()\n",
    "                    central_x = (sample['boxes'][idx_gt, 0] + sample['boxes'][idx_gt, 2]) // 2 + torch.randint(-10, 10, (1,)).item()\n",
    "                    central_y = (sample['boxes'][idx_gt, 1] + sample['boxes'][idx_gt, 3]) // 2 + torch.randint(-10, 10, (1,)).item()\n",
    "\n",
    "                x1 = np.clip(central_x - self.box_size // 2, 0, img.shape[2] - self.box_size)\n",
    "                y1 = np.clip(central_y - self.box_size // 2, 0, img.shape[1] - self.box_size)\n",
    "                x2, y2 = x1 + self.box_size, y1 + self.box_size\n",
    "\n",
    "                crop = img[:, y1:y2, x1:x2]\n",
    "                label = self.get_label_of_crop((x1, y1, x2, y2), sample['boxes'], sample['labels'])\n",
    "\n",
    "                crops.append(crop)\n",
    "                labels.append(label)\n",
    "\n",
    "        return {\n",
    "            \"crops\": torch.stack(crops, dim=0),\n",
    "            \"labels\": torch.tensor(labels)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YcHcqTppTM5r"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "# kod do walidacji metodą okna przesuwnego\n",
    "\n",
    "class SlidingWindowDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Klasa implementująca detektor obiektów za pomocą metody okna przesuwnego. Jest to przykładowe rozwiązanie problemu detekcji monet.\n",
    "\n",
    "    Przyjmuje:\n",
    "        classifier (nn.Module): Model klasyfikatora obiektów. Dla każdej pozycji okna przesuwnego,\n",
    "                    klasyfikator zwraca predykcję dla danego wycinka obrazu.\n",
    "        crop_size: Rozmiar okna przesuwnego\n",
    "        stride: dystans o jaki przesuwamy okno w każdej iteracji.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            classifier: nn.Module,\n",
    "            crop_size: int = 64,\n",
    "            stride: int = 32\n",
    "        ):\n",
    "        super(SlidingWindowDetector, self).__init__()\n",
    "        self.classifier = classifier\n",
    "        self.crop_size = crop_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            image: torch.Tensor\n",
    "        ) -> list:\n",
    "        \"\"\"\n",
    "        Funkcja przewidująca obiekty na obrazie za pomocą metody okna przesuwnego.\n",
    "\n",
    "        Przyjmuje:\n",
    "            image: Obraz do przetworzenia.\n",
    "\n",
    "        Zwraca:\n",
    "            lista znalezionych obiektów w formie krotki (x1, y1, x2, y2, label, confidence).\n",
    "        \"\"\"\n",
    "        # przenosimy obraz na odpowiednie urządzenie (takie jak model)\n",
    "        device = next(self.parameters()).device\n",
    "        image = image.to(device)\n",
    "\n",
    "        found_objects = [] # lista obiektów, gdzie obiektem jest krotka (x1, y1, x2, y2, label, confidence)\n",
    "\n",
    "        # przesuwamy okno po obrazie\n",
    "        for y in range(0, image.shape[1] - self.crop_size, self.stride):\n",
    "            for x in range(0, image.shape[2] - self.crop_size, self.stride):\n",
    "                crop = image[:, y:y+self.crop_size, x:x+self.crop_size]\n",
    "                pred = self.classifier(crop.unsqueeze(0))[0]\n",
    "\n",
    "                if pred.argmax() != 9: # tło pomijamy\n",
    "                    label = pred.argmax().item()\n",
    "                    confidence = torch.softmax(pred, dim=0)[label].item()\n",
    "                    found_objects.append((x, y, x+self.crop_size, y+self.crop_size, label, confidence))\n",
    "\n",
    "        return found_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3f0jVWEQTM5s"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "def train_basic_detector(\n",
    "        train_ds: Dataset\n",
    "    ) -> SlidingWindowDetector:\n",
    "    \"\"\"\n",
    "    Funkcja trenująca klasyfikator monet za pomocą tworzenia wycinków obrazów i klasyfikowania ich.\n",
    "    Gdy klasyfikator jest wytrenowany, tworzymy detektor obiektów za pomocą metody okna przesuwnego (SlidingWindowDetector).\n",
    "\n",
    "    Przyjmuje:\n",
    "        train_ds (Dataset): Zbiór danych treningowych.\n",
    "\n",
    "    Zwraca:\n",
    "        SlidingWindowDetector: Wytrenowany detektor obiektów korzystający z metody okna przesuwnego.\n",
    "    \"\"\"\n",
    "\n",
    "    # jaki powinien być rozmiar okna przesuwnego? odpowiedź na to pytanie znajdziemy w danych...\n",
    "    # wyznaczmy średni rozmiar wszystkich obiektów w zbiorze treningowym\n",
    "    total = 0\n",
    "    boxes = 0\n",
    "\n",
    "    for sample in train_ds:\n",
    "        total += sum(sample[\"boxes\"][:, 2] - sample[\"boxes\"][:, 0]) # x2 - x1 (zakładamy, że obiekty są kwadratowe, dlatego pomijamy y)\n",
    "        boxes += len(sample[\"boxes\"])\n",
    "\n",
    "    box_size = (total / boxes).item()\n",
    "\n",
    "    print(\"Średni rozmiar obiektu w zbiorze treningowym:\", box_size) # powinno wyjść około 62\n",
    "\n",
    "    # ta wartość jest zbliżona do wartości 64, która jest potęgą liczby 2, co ułatwi nam architekturę sieci\n",
    "    box_size = 64\n",
    "    preprocess = ClassificationDataPreprocessor(num_of_crops_per_image=128, box_size=box_size)\n",
    "\n",
    "    # przygotujmy model, optymalizator oraz funkcję straty (dzięki skorzystaniu z techniki okna przesuwnego problem uczenia naszego modelu stał się problemem klasyfikacji)\n",
    "    model = BasicCNNClassifier()\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # przygotujmy dataloadery\n",
    "    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=preprocess)\n",
    "\n",
    "    # trenujmy model przez wybraną liczbę epok\n",
    "    epochs = 30\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"Training\", total=epochs)\n",
    "    for _ in pbar:\n",
    "        epoch_losses = []\n",
    "\n",
    "        for batch in train_dl:\n",
    "            X = batch[\"crops\"].to(DEVICE)\n",
    "            y = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.detach().cpu().item())\n",
    "\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        pbar.set_postfix({'train loss': avg_loss})\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # konstruujemy detektor obiektów za pomocą metody okna przesuwnego, który używa wytrenowanego klasyfikatora do predykcji wycinków\n",
    "    return SlidingWindowDetector(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IpvuHrb7TM5t"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    train_ds, val_ds = setup_data(root='data/')\n",
    "\n",
    "    model = train_basic_detector(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NbPeudfaTM5t"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    out = predict_all_bounding_boxes(model, val_ds)\n",
    "\n",
    "    map_val = calculate_map(out, val_ds, return_all=True)\n",
    "\n",
    "    print(f\"mAP z użyciem progu IoU=0.5 na zbiorze walidacyjnym: {map_val['map_50'].item():.2f}\")\n",
    "    print(f\"mAP dla wielu wartości IoU na zbiorze walidacyjnym:  {map_val['map']:.2f}, to jest metryka, która podlega ocenie w konkursie. Twoim zadaniem jest ją maksymalizować.\")\n",
    "\n",
    "    plot_detection_results_grid(out, val_ds)\n",
    "\n",
    "    plot_confusion_matrix(out, val_ds, iou_threshold=0.5)\n",
    "    plot_confusion_matrix(out, val_ds, iou_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ro0G4uSTM5u"
   },
   "source": [
    "Warto zwrócić uwagę na rozbieżności w wartościach mAP z użyciem progu IoU=0.5 oraz dla wielu różnych wartości IoU (0.5, ..., 0.95). Podobna rozbieżność jest widoczna w macierzy pomyłek dla różnych wartości IoU. Oznacza to, że model potrafi całkiem dobrze predykować klasy, jednak ma problem z dokładną lokalizacją obiektów.\n",
    "\n",
    "Jest to spodziewany wynik, ponieważ model używa techniki okna przesuwnego, która nie umieszcza prostokąta dokładnie na monecie, a jedynie przesuwa je o konkretną wartość (u nas 32 piksele), co sprawia, że proponowane przez model pozycje są jedynie w okolicach prawdziwych monet. Dodatkowo wielkość zwracanych okien jest stała, co dodatkowo pogarsza wyniki mAP dla wyższych wartości progu IoU.\n",
    "\n",
    "Twoim zadaniem jest zaimplementowanie modelu, który poprawi te wyniki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrsIalZLTM5u"
   },
   "source": [
    "# Twoje Rozwiązanie\n",
    "W tej sekcji należy umieścić Twoje rozwiązanie. Wprowadzaj zmiany wyłącznie tutaj!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podział datasetu na patche 200x200, binarna segmentację tło/moneta (przy czym model musi być naprawdę pewny że dany piksel to moneta - threashold = 0.9 i jednogłośność decyzji dla 4 różnych perspektyw), dalej detekcja bounding boxów i klasyfikację monet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "W1h4cPQyTM5u"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# tutaj możesz zaimplementować swój detektor, który będzie zwracał listę znalezionych obiektów w formacie (x1, y1, x2, y2, label, confidence)\n",
    "\n",
    "def circle_mask(tensor, O_x, O_y, r):\n",
    "    coord_x, coord_y = tensor.shape\n",
    "    x = torch.arange(coord_x).view(-1, 1).expand(-1, coord_y)\n",
    "    y = torch.arange(coord_y).view(1, -1).expand(coord_x, -1)\n",
    "\n",
    "    return (x - O_x) ** 2 + (y - O_y) ** 2 <= r ** 2\n",
    "\n",
    "class MyClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyClassifier, self).__init__()\n",
    "    self.seq = nn.Sequential(\n",
    "      nn.Conv2d(\n",
    "        in_channels=3,\n",
    "        out_channels=8,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1\n",
    "      ),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      nn.LeakyReLU(),\n",
    "\n",
    "      nn.Conv2d(\n",
    "        in_channels=8,\n",
    "        out_channels=16,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1\n",
    "      ),\n",
    "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "      nn.LeakyReLU(),\n",
    "\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(4624, 9)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.seq(x)\n",
    "\n",
    "class MySegmenter(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MySegmenter, self).__init__()\n",
    "    self.seq = nn.Sequential(\n",
    "        nn.Conv2d(3, 4, 3, 2, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(4),\n",
    "\n",
    "        nn.Conv2d(4, 4, 3, 1, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(4),\n",
    "\n",
    "        nn.Conv2d(4, 8, 3, 2, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8),\n",
    "\n",
    "        nn.Conv2d(8, 8, 3, 1, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8),\n",
    "\n",
    "        nn.Conv2d(8, 16, 3, 2, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16),\n",
    "\n",
    "        nn.Conv2d(16, 16, 3, 1, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16),\n",
    "\n",
    "        nn.Conv2d(16, 32, 3, 1, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(32),\n",
    "\n",
    "        nn.Conv2d(32, 32, 3, 1, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(32),\n",
    "        # jeszcze jedna?\n",
    "\n",
    "        nn.ConvTranspose2d(32, 16, 3, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(16),\n",
    "\n",
    "        nn.Conv2d(16, 8, 3, 1, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8),\n",
    "\n",
    "        nn.Conv2d(8, 8, 3, 1, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(8),\n",
    "\n",
    "        nn.ConvTranspose2d(8, 4, 3, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(4),\n",
    "\n",
    "        nn.Conv2d(4, 4, 3, 1, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(4),\n",
    "\n",
    "        nn.ConvTranspose2d(4, 1, 3, 2),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x[:, :, 0:200, 0:200]\n",
    "\n",
    "# głosowanie nad pixelem z 4 różnych układów\n",
    "\n",
    "def decide(tens1, tens2, tens3, tens4):\n",
    "  sm = lambda x: nn.functional.softmax(x, dim=-1)\n",
    "  v, i = torch.stack([sm(tens1), sm(tens2), sm(tens3), sm(tens4)]).max(dim=0)\n",
    "\n",
    "  return v.argmax(-1), v\n",
    "\n",
    "def findContoursVanilla(mask):\n",
    "    \"\"\"\n",
    "    Find connected components in 'mask' (0/1 array) using BFS.\n",
    "    Returns a list of 'contours', where each 'contour' is a list\n",
    "    of (row, col) pixels belonging to one connected component.\n",
    "    \"\"\"\n",
    "    visited = np.zeros_like(mask, dtype=bool)\n",
    "    rows, cols = mask.shape\n",
    "    contours = []\n",
    "    directions = [(-1,0), (1,0), (0,-1), (0,1)]  # 4-connectivity\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if mask[r, c] == 1 and not visited[r, c]:\n",
    "                q = deque()\n",
    "                q.append((r, c))\n",
    "                visited[r, c] = True\n",
    "                component_pixels = []\n",
    "\n",
    "                while q:\n",
    "                    rr, cc = q.popleft()\n",
    "                    component_pixels.append((rr, cc))\n",
    "\n",
    "                    for dr, dc in directions:\n",
    "                        nr, nc = rr + dr, cc + dc\n",
    "                        if 0 <= nr < rows and 0 <= nc < cols:\n",
    "                            if mask[nr, nc] == 1 and not visited[nr, nc]:\n",
    "                                visited[nr, nc] = True\n",
    "                                q.append((nr, nc))\n",
    "\n",
    "                contours.append(component_pixels)\n",
    "\n",
    "    return contours\n",
    "\n",
    "def boundingRectVanilla(component):\n",
    "    \"\"\"\n",
    "    Compute the bounding box of all pixels in 'component'.\n",
    "    Returns (x, y, w, h) just like cv2.boundingRect would.\n",
    "    \"\"\"\n",
    "    # component is a list of (row, col)\n",
    "    rows = [p[0] for p in component]\n",
    "    cols = [p[1] for p in component]\n",
    "    min_r, max_r = min(rows), max(rows)\n",
    "    min_c, max_c = min(cols), max(cols)\n",
    "    x = min_c\n",
    "    y = min_r\n",
    "    w = (max_c - min_c + 1)\n",
    "    h = (max_r - min_r + 1)\n",
    "    return x, y, w, h\n",
    "\n",
    "class YourDetector(nn.Module):\n",
    "    def __init__(self, trained_classifier, trained_segmenter):\n",
    "        # tutaj możesz zainicjalizować swój model\n",
    "        super(YourDetector, self).__init__()\n",
    "        self.cls = trained_classifier\n",
    "        self.seg = trained_segmenter\n",
    "\n",
    "        self.cls.eval()\n",
    "        self.seg.eval()\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        img: torch.Tensor\n",
    "    ) -> list:\n",
    "        seg_map = torch.ones((1, img.shape[1], img.shape[2])).to(DEVICE)\n",
    "\n",
    "        stride = 200\n",
    "        x = 520\n",
    "        y = 640\n",
    "\n",
    "        ts = 0.7\n",
    "        sg = nn.Sigmoid()\n",
    "\n",
    "        patches_list = [\n",
    "            img[None, :, 0:200, 0:200],\n",
    "            img[None, :, 0:200, 200:400],\n",
    "            img[None, :, 0:200, 400:600],\n",
    "            img[None, :, 200:400, 0:200],\n",
    "            img[None, :, 200:400, 200:400],\n",
    "            img[None, :, 200:400, 400:600],\n",
    "            img[None, :, 320:520, 0:200],\n",
    "            img[None, :, 320:520, 200:400],\n",
    "            img[None, :, 320:520, 400:600],\n",
    "        ]\n",
    "\n",
    "        # I run do ratation for every patch to make the results more robust\n",
    "        original_len = len(patches_list)\n",
    "        for j in range(3):\n",
    "          for i in range(original_len):\n",
    "            patches_list.append(patches_list[i].rot90(j+1, [2, 3]))\n",
    "\n",
    "        patch_masks = sg(self.seg(torch.cat(patches_list)))\n",
    "        patch_masks = patch_masks.reshape((4, 9, 200, 200))\n",
    "\n",
    "        for j in range(3):\n",
    "          patch_masks[j+1] = patch_masks[j+1].rot90(-(j+1), [1, 2])\n",
    "\n",
    "        patch_masks = patch_masks.sum(0) / 4 > ts\n",
    "\n",
    "        seg_map[:, 0:200, 0:200] *= patch_masks[0]\n",
    "        seg_map[:, 0:200, 200:400] *= patch_masks[1]\n",
    "        seg_map[:, 0:200, 400:600] *= patch_masks[2]\n",
    "\n",
    "        # second row\n",
    "        seg_map[:, 200:400, 0:200] *= patch_masks[3]\n",
    "        seg_map[:, 200:400, 200:400] *= patch_masks[4]\n",
    "        seg_map[:, 200:400, 400:600] *= patch_masks[5]\n",
    "\n",
    "        # last row (overlap)\n",
    "        seg_map[:, 320:520, 0:200] *= patch_masks[6]\n",
    "        seg_map[:, 320:520, 200:400] *= patch_masks[7]\n",
    "        seg_map[:, 320:520, 400:600] *= patch_masks[8]\n",
    "\n",
    "        # this right stripe\n",
    "        seg_map[:, :, 600:640] = 0\n",
    "\n",
    "\n",
    "        plt.imshow(img.permute(1, 2, 0).cpu())\n",
    "        plt.imshow(seg_map.permute(1, 2, 0).cpu(), alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        # chatgpt (my skill issue)\n",
    "        seg_map = seg_map.cpu().numpy()\n",
    "        binary_mask = seg_map.squeeze()\n",
    "        binary_mask = binary_mask.astype(np.uint8)\n",
    "\n",
    "        contours = findContoursVanilla(binary_mask)\n",
    "\n",
    "        results = []\n",
    "        coin_squares = []\n",
    "\n",
    "        for cntr in contours:\n",
    "            x, y, w, h = boundingRectVanilla(cntr)\n",
    "            if w < 20 or h < 20:\n",
    "                continue\n",
    "\n",
    "            O = (int(x + w//2), int(y + h//2))\n",
    "            r = (w + h)//4\n",
    "\n",
    "            sq_x, sq_y = O[0] - r, O[1] - r\n",
    "            coin_square = rs(img[:, sq_y : sq_y + 2*r, sq_x : sq_x + 2*r])\n",
    "            results.append([x, y, x + w, y + h])\n",
    "            coin_squares.append(coin_square)\n",
    "\n",
    "\n",
    "        X = torch.stack(coin_squares)\n",
    "        coin_mask = circle_mask(X[0][0], X.shape[2]//2, X.shape[2]//2, X.shape[2]//2).to(DEVICE)\n",
    "        X *= coin_mask[None, None, :, :]\n",
    "\n",
    "        out1 = self.cls(X)\n",
    "        out2 = self.cls(torch.rot90(X, 1, [2, 3]))\n",
    "        out3 = self.cls(torch.rot90(X, 2, [2, 3]))\n",
    "        out4 = self.cls(torch.rot90(X, 3, [2, 3]))\n",
    "\n",
    "        labels, prob_tensor = decide(out1, out2, out3, out4)\n",
    "\n",
    "        for result, label, prob in zip(results, labels, prob_tensor):\n",
    "          result.append(label.item())\n",
    "          result.append(prob[label].item())\n",
    "\n",
    "        return results\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "            self,\n",
    "            img: torch.Tensor\n",
    "        ) -> list:\n",
    "\n",
    "        try:\n",
    "          return self.predict(img)\n",
    "        except:\n",
    "          return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lntuHZwrTM5u"
   },
   "outputs": [],
   "source": [
    "# definicje augmentacji dla zbioru treningowego i walidacyjnego. Domyślnie None oznacza brak augmentacji\n",
    "train_transform = None\n",
    "val_transform = None\n",
    "\n",
    "train_ds, val_ds = setup_data(train_transform, val_transform, root='data/')\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "IMG_EDGE_SIZE = 70\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "rs = T.Resize((IMG_EDGE_SIZE, IMG_EDGE_SIZE))\n",
    "rvf = T.RandomVerticalFlip(p=1)\n",
    "rr = T.RandomRotation(degrees=360)\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import copy\n",
    "import random\n",
    "from torchmetrics import F1Score\n",
    "from collections import deque\n",
    "\n",
    "def valid_cls_model(model, valid_dl_cls, criterion):\n",
    "  model.eval()\n",
    "  valid_losses = []\n",
    "  real_labels = []\n",
    "  model_labels = []\n",
    "  with torch.no_grad():\n",
    "    for step, (X, y) in enumerate(valid_dl_cls):\n",
    "      X = X.to(DEVICE)\n",
    "      y = y.to(DEVICE)\n",
    "\n",
    "      out1 = model(X)\n",
    "      valid_losses.append(criterion(out1, y))\n",
    "\n",
    "      out2 = model(torch.rot90(X, 1, [2, 3]))\n",
    "      valid_losses.append(criterion(out2, y))\n",
    "\n",
    "      out3 = model(torch.rot90(X, 2, [2, 3]))\n",
    "      valid_losses.append(criterion(out3, y))\n",
    "\n",
    "      out4 = model(torch.rot90(X, 3, [2, 3]))\n",
    "      valid_losses.append(criterion(out4, y))\n",
    "\n",
    "      labels, prob_tensor = decide(out1, out2, out3, out4)\n",
    "\n",
    "      real_labels.append(y)\n",
    "      model_labels.append(labels)\n",
    "\n",
    "  return (torch.cat(real_labels) == torch.cat(model_labels)).float().mean().item()\n",
    "\n",
    "class ClassificationDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, detection_dataset):\n",
    "    self.cls_dataset = []\n",
    "    for item in detection_dataset: self.cls_dataset += self._detection_to_cls(item)\n",
    "\n",
    "    self.transforms = [\n",
    "        lambda x: x,\n",
    "        lambda img: torch.rot90(img, 1, [1, 2]),\n",
    "        lambda img: torch.rot90(img, 2, [1, 2]),\n",
    "        lambda img: torch.rot90(img, 3, [1, 2]),\n",
    "    ]\n",
    "\n",
    "    self.L = len(self.transforms)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    big_idx = idx // self.L\n",
    "    small_idx = idx % self.L\n",
    "    return (\n",
    "        self.transforms[small_idx](self.cls_dataset[big_idx][0]),\n",
    "        self.cls_dataset[big_idx][1],\n",
    "    )\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.cls_dataset) * self.L\n",
    "\n",
    "  def _detection_to_cls(self, item):\n",
    "    new_data = []\n",
    "\n",
    "    image = item[\"image\"]\n",
    "    boxes = item[\"boxes\"]\n",
    "    labels = item[\"labels\"]\n",
    "\n",
    "    # resize, przesuwanie itp\n",
    "    for box, label in zip(boxes, labels):\n",
    "      img = image[:, box[1]:box[3], box[0]:box[2]]\n",
    "      img = rs(img)\n",
    "\n",
    "      coin_mask = circle_mask(img[0], img.shape[2]//2, img.shape[2]//2, img.shape[2]//2)\n",
    "      img *= coin_mask[None, :, :]\n",
    "\n",
    "      new_data.append((img, label))\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def train_cls_model(\n",
    "  BATCH_SIZE,\n",
    "  IMG_EDGE_SIZE,\n",
    "  LR,\n",
    "  NUM_EPOCHS,\n",
    "):\n",
    "  best_model = None\n",
    "  best_accuracy = 0\n",
    "\n",
    "  train_ds_cls = ClassificationDataset(train_ds)\n",
    "  train_dl_cls = torch.utils.data.DataLoader(train_ds_cls, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "  valid_ds_cls = ClassificationDataset(val_ds)\n",
    "  valid_dl_cls = torch.utils.data.DataLoader(valid_ds_cls, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "  model = MyClassifier().to(DEVICE)\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "  model.train()\n",
    "  for _ in range(NUM_EPOCHS):\n",
    "    for X, y in train_dl_cls:\n",
    "      X = X.to(DEVICE)\n",
    "      y = y.to(DEVICE)\n",
    "      optimizer.zero_grad()\n",
    "      out = model(X)\n",
    "      loss = criterion(out, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    v = valid_cls_model(model, valid_dl_cls, criterion)\n",
    "    if v > best_accuracy:\n",
    "      best_accuracy = v\n",
    "      best_model = copy.deepcopy(model)\n",
    "\n",
    "  del train_ds_cls, valid_ds_cls, train_dl_cls, valid_dl_cls\n",
    "  return best_model, best_accuracy\n",
    "\n",
    "cls_model, best_accuracy = train_cls_model(\n",
    "  BATCH_SIZE = BATCH_SIZE,\n",
    "  IMG_EDGE_SIZE = IMG_EDGE_SIZE,\n",
    "  LR = LR,\n",
    "  NUM_EPOCHS = NUM_EPOCHS,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "def decision(probability):\n",
    "    return random.random() < probability\n",
    "\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, detection_dataset):\n",
    "        self.detection_dataset = []\n",
    "        for item in detection_dataset:\n",
    "            self.detection_dataset += self._detection_to_segmentation(item)\n",
    "        # Używamy tylko dwóch augmentacji: brak zmiany oraz obrót o 90 stopni\n",
    "        self.transforms = [\n",
    "            lambda x: x,\n",
    "            lambda x: torch.rot90(x, 1, [1, 2]),\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.detection_dataset) * 2  # Zmiana: mnożymy przez 2 zamiast 4\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        big_idx = idx // 2\n",
    "        small_idx = idx % 2\n",
    "        return (\n",
    "            self.transforms[small_idx](self.detection_dataset[big_idx][0]),\n",
    "            self.transforms[small_idx](self.detection_dataset[big_idx][1])\n",
    "        )\n",
    "\n",
    "    def _detection_to_segmentation(self, item):\n",
    "        image = item[\"image\"]\n",
    "        boxes = item[\"boxes\"]\n",
    "        labels = item[\"labels\"]\n",
    "\n",
    "        coord_x, coord_y = image.shape[1], image.shape[2]\n",
    "\n",
    "        segmentation_map = torch.zeros((coord_x, coord_y))\n",
    "\n",
    "        # segmenting all images\n",
    "        for box, label in zip(boxes, labels):\n",
    "            img = image[:, box[1]:box[3], box[0]:box[2]]\n",
    "\n",
    "            O = (box[1] + box[3]) / 2, (box[0] + box[2]) / 2\n",
    "            r = ((torch.abs(box[0] - box[2]) + torch.abs(box[1] - box[3])) / 4)\n",
    "            ring_r = r * 1.4\n",
    "\n",
    "            ring_mask = circle_mask(segmentation_map, O[0], O[1], ring_r)\n",
    "            segmentation_map[ring_mask] = 0\n",
    "\n",
    "            mask = circle_mask(segmentation_map, O[0], O[1], r)\n",
    "            segmentation_map[mask] = 1\n",
    "\n",
    "        # creating patches for binary segmentation\n",
    "        size = 200\n",
    "        stride = 40\n",
    "        propability = 1\n",
    "\n",
    "        img_patches = image.unfold(1, size, stride).unfold(2, size, stride).unfold(3, size, stride).flatten(start_dim=1, end_dim=3).movedim(1, 0)\n",
    "        map_patches = segmentation_map[None, :, :].unfold(1, size, stride).unfold(2, size, stride).unfold(3, size, stride).flatten(start_dim=1, end_dim=3).movedim(1, 0)\n",
    "\n",
    "        ds_from_image = []\n",
    "        for img_patch, map_patch in zip(img_patches, map_patches):\n",
    "            if map_patch.sum() > 0 or decision(propability):\n",
    "                ds_from_image.append((img_patch, map_patch))\n",
    "\n",
    "        return ds_from_image\n",
    "\n",
    "# Przykład użycia:\n",
    "train_ds_seg = SegmentationDataset(train_ds)\n",
    "valid_ds_seg = SegmentationDataset(val_ds)\n",
    "\n",
    "train_dl_seg = torch.utils.data.DataLoader(train_ds_seg, BATCH_SIZE, shuffle=True)\n",
    "valid_dl_seg = torch.utils.data.DataLoader(valid_ds_seg, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "positive = sum([x[1].sum() for x in valid_ds_seg])\n",
    "negative = len(valid_ds_seg)*200*200 - positive\n",
    "\n",
    "f1 = F1Score(task='binary').to(DEVICE)\n",
    "\n",
    "model = MySegmenter().to(DEVICE)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=negative/positive)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "def eval_seg_model(model, valid_dl_cls, show):\n",
    "  model.eval()\n",
    "  valid_losses = []\n",
    "  real_labels = []\n",
    "  model_labels = []\n",
    "  with torch.no_grad():\n",
    "    for step, (X, y) in enumerate(valid_dl_cls):\n",
    "      X = X.to(DEVICE)\n",
    "      y = y.to(DEVICE)\n",
    "      out = model(X)\n",
    "      loss = criterion(out, y)\n",
    "      valid_losses.append(loss)\n",
    "\n",
    "      real_labels.append(y.flatten(start_dim=1))\n",
    "      model_labels.append((nn.functional.sigmoid(out).flatten(start_dim=1) > 0.90).int())\n",
    "\n",
    "  return f1(torch.cat(model_labels), torch.cat(real_labels)).item()\n",
    "\n",
    "best_f1 = 0\n",
    "best_model = None\n",
    "for i in range(20):\n",
    "  model.train()\n",
    "  for X, y in train_dl_seg:\n",
    "    X = X.to(DEVICE)\n",
    "    y = y.to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(X)\n",
    "    loss = criterion(out, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  f1_res = eval_seg_model(model, valid_dl_seg, (i+1)%5 == 0)\n",
    "  if f1_res > best_f1:\n",
    "    best_f1 = f1_res\n",
    "    best_model = copy.deepcopy(model)\n",
    "\n",
    "del train_ds_seg\n",
    "del valid_ds_seg\n",
    "\n",
    "del train_dl_seg\n",
    "del valid_dl_seg\n",
    "\n",
    "def findContoursVanilla(mask):\n",
    "    \"\"\"\n",
    "    Find connected components in 'mask' (0/1 array) using BFS.\n",
    "    Returns a list of 'contours', where each 'contour' is a list\n",
    "    of (row, col) pixels belonging to one connected component.\n",
    "    \"\"\"\n",
    "    visited = np.zeros_like(mask, dtype=bool)\n",
    "    rows, cols = mask.shape\n",
    "    contours = []\n",
    "    directions = [(-1,0), (1,0), (0,-1), (0,1)]  # 4-connectivity\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if mask[r, c] == 1 and not visited[r, c]:\n",
    "                q = deque()\n",
    "                q.append((r, c))\n",
    "                visited[r, c] = True\n",
    "                component_pixels = []\n",
    "\n",
    "                while q:\n",
    "                    rr, cc = q.popleft()\n",
    "                    component_pixels.append((rr, cc))\n",
    "\n",
    "                    for dr, dc in directions:\n",
    "                        nr, nc = rr + dr, cc + dc\n",
    "                        if 0 <= nr < rows and 0 <= nc < cols:\n",
    "                            if mask[nr, nc] == 1 and not visited[nr, nc]:\n",
    "                                visited[nr, nc] = True\n",
    "                                q.append((nr, nc))\n",
    "\n",
    "                contours.append(component_pixels)\n",
    "\n",
    "    return contours\n",
    "\n",
    "def boundingRectVanilla(component):\n",
    "    \"\"\"\n",
    "    Compute the bounding box of all pixels in 'component'.\n",
    "    Returns (x, y, w, h) just like cv2.boundingRect would.\n",
    "    \"\"\"\n",
    "    # component is a list of (row, col)\n",
    "    rows = [p[0] for p in component]\n",
    "    cols = [p[1] for p in component]\n",
    "    min_r, max_r = min(rows), max(rows)\n",
    "    min_c, max_c = min(cols), max(cols)\n",
    "    x = min_c\n",
    "    y = min_r\n",
    "    w = (max_c - min_c + 1)\n",
    "    h = (max_r - min_r + 1)\n",
    "    return x, y, w, h\n",
    "\n",
    "class YourDetector(nn.Module):\n",
    "    def __init__(self, trained_classifier, trained_segmenter):\n",
    "        # tutaj możesz zainicjalizować swój model\n",
    "        super(YourDetector, self).__init__()\n",
    "        self.cls = trained_classifier\n",
    "        self.seg = trained_segmenter\n",
    "\n",
    "        self.cls.eval()\n",
    "        self.seg.eval()\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        img: torch.Tensor\n",
    "    ) -> list:\n",
    "        seg_map = torch.ones((1, img.shape[1], img.shape[2])).to(DEVICE)\n",
    "\n",
    "        stride = 200\n",
    "        x = 520\n",
    "        y = 640\n",
    "\n",
    "        ts = 0.7\n",
    "        sg = nn.Sigmoid()\n",
    "\n",
    "        patches_list = [\n",
    "            img[None, :, 0:200, 0:200],\n",
    "            img[None, :, 0:200, 200:400],\n",
    "            img[None, :, 0:200, 400:600],\n",
    "            img[None, :, 200:400, 0:200],\n",
    "            img[None, :, 200:400, 200:400],\n",
    "            img[None, :, 200:400, 400:600],\n",
    "            img[None, :, 320:520, 0:200],\n",
    "            img[None, :, 320:520, 200:400],\n",
    "            img[None, :, 320:520, 400:600],\n",
    "        ]\n",
    "\n",
    "        # I run do ratation for every patch to make the results more robust\n",
    "        original_len = len(patches_list)\n",
    "        for j in range(3):\n",
    "          for i in range(original_len):\n",
    "            patches_list.append(patches_list[i].rot90(j+1, [2, 3]))\n",
    "\n",
    "        patch_masks = sg(self.seg(torch.cat(patches_list)))\n",
    "        patch_masks = patch_masks.reshape((4, 9, 200, 200))\n",
    "\n",
    "        for j in range(3):\n",
    "          patch_masks[j+1] = patch_masks[j+1].rot90(-(j+1), [1, 2])\n",
    "\n",
    "        patch_masks = patch_masks.sum(0) / 4 > ts\n",
    "\n",
    "        seg_map[:, 0:200, 0:200] *= patch_masks[0]\n",
    "        seg_map[:, 0:200, 200:400] *= patch_masks[1]\n",
    "        seg_map[:, 0:200, 400:600] *= patch_masks[2]\n",
    "\n",
    "        # second row\n",
    "        seg_map[:, 200:400, 0:200] *= patch_masks[3]\n",
    "        seg_map[:, 200:400, 200:400] *= patch_masks[4]\n",
    "        seg_map[:, 200:400, 400:600] *= patch_masks[5]\n",
    "\n",
    "        # last row (overlap)\n",
    "        seg_map[:, 320:520, 0:200] *= patch_masks[6]\n",
    "        seg_map[:, 320:520, 200:400] *= patch_masks[7]\n",
    "        seg_map[:, 320:520, 400:600] *= patch_masks[8]\n",
    "\n",
    "        # this right stripe\n",
    "        seg_map[:, :, 600:640] = 0\n",
    "\n",
    "\n",
    "        plt.imshow(img.permute(1, 2, 0).cpu())\n",
    "        plt.imshow(seg_map.permute(1, 2, 0).cpu(), alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        # chatgpt (my skill issue)\n",
    "        seg_map = seg_map.cpu().numpy()\n",
    "        binary_mask = seg_map.squeeze()\n",
    "        binary_mask = binary_mask.astype(np.uint8)\n",
    "\n",
    "        contours = findContoursVanilla(binary_mask)\n",
    "\n",
    "        results = []\n",
    "        coin_squares = []\n",
    "\n",
    "        for cntr in contours:\n",
    "            x, y, w, h = boundingRectVanilla(cntr)\n",
    "            if w < 20 or h < 20:\n",
    "                continue\n",
    "\n",
    "            O = (int(x + w//2), int(y + h//2))\n",
    "            r = (w + h)//4\n",
    "\n",
    "            sq_x, sq_y = O[0] - r, O[1] - r\n",
    "            coin_square = rs(img[:, sq_y : sq_y + 2*r, sq_x : sq_x + 2*r])\n",
    "            results.append([x, y, x + w, y + h])\n",
    "            coin_squares.append(coin_square)\n",
    "\n",
    "\n",
    "        X = torch.stack(coin_squares)\n",
    "        coin_mask = circle_mask(X[0][0], X.shape[2]//2, X.shape[2]//2, X.shape[2]//2).to(DEVICE)\n",
    "        X *= coin_mask[None, None, :, :]\n",
    "\n",
    "        out1 = self.cls(X)\n",
    "        out2 = self.cls(torch.rot90(X, 1, [2, 3]))\n",
    "        out3 = self.cls(torch.rot90(X, 2, [2, 3]))\n",
    "        out4 = self.cls(torch.rot90(X, 3, [2, 3]))\n",
    "\n",
    "        labels, prob_tensor = decide(out1, out2, out3, out4)\n",
    "\n",
    "        for result, label, prob in zip(results, labels, prob_tensor):\n",
    "          result.append(label.item())\n",
    "          result.append(prob[label].item())\n",
    "\n",
    "        return results\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "            self,\n",
    "            img: torch.Tensor\n",
    "        ) -> list:\n",
    "\n",
    "        try:\n",
    "          return self.predict(img)\n",
    "        except:\n",
    "          return []\n",
    "\n",
    "your_model = YourDetector(cls_model, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSna1-pZuSco"
   },
   "source": [
    "# Ewaluacja\n",
    "\n",
    "Uruchomienie poniższej komórki pozwoli sprawdzić, ile punktów zdobyłoby Twoje rozwiązanie na danych walidacyjnych. Przed wysłaniem upewnij się, że cały notebook wykonuje się od początku do końca bez błędów i bez konieczności ingerencji użytkownika po wybraniu opcji \"Run All\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "axcBXOclTM5v"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    your_out = predict_all_bounding_boxes(your_model, val_ds)\n",
    "\n",
    "    your_map_val = calculate_map(your_out, val_ds, return_all=False) # zwracamy tylko uśrednione mAP (główne kryterium oceny)\n",
    "\n",
    "    score = (np.clip(your_map_val, 0.2, 0.85) - 0.2) / 0.65 * 100\n",
    "    score = int(round(score))\n",
    "\n",
    "    print(f\"mAP na zbiorze walidacyjnym: {your_map_val:.2f}\")\n",
    "    print(f\"Estymowana liczba punktów za zadanie: {score}\")\n",
    "\n",
    "    plot_detection_results_grid(your_out, val_ds)\n",
    "\n",
    "    plot_confusion_matrix(your_out, val_ds, iou_threshold=0.5)\n",
    "    plot_confusion_matrix(your_out, val_ds, iou_threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06u0AB8fuSco"
   },
   "source": [
    "Podczas sprawdzania model zostanie zapisany jako `your_model.pkl` i oceniony na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "v2E-8xSTrIVt"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if FINAL_EVALUATION_MODE:\n",
    "    import cloudpickle\n",
    "\n",
    "    # Gdy model posiada parametry, ustaw go w trybie ewaluacji i przenieś na CPU\n",
    "    if list(your_model.parameters()):\n",
    "        your_model.eval()\n",
    "        your_model.cpu()\n",
    "\n",
    "    OUTPUT_PATH = \"file_output\"\n",
    "    FUNCTION_FILENAME = \"your_model.pkl\"\n",
    "    FUNCTION_OUTPUT_PATH = os.path.join(OUTPUT_PATH, FUNCTION_FILENAME)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "    with open(FUNCTION_OUTPUT_PATH, \"wb\") as f:\n",
    "        cloudpickle.dump(your_model, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
